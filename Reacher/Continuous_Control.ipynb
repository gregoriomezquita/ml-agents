{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG - Reacher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "aws_unity_filename= './Reacher_Linux_NoVis/Reacher.x86_64'\n",
    "laptop_unity_filename= './Reacher_Linux/Reacher.x86_64'\n",
    "seed= 0 #datetime.now().second\n",
    "env = UnityEnvironment(seed= seed, file_name= laptop_unity_filename)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "#Handy functions to help understand the code\n",
    "def env_reset(env, mode=True):\n",
    "    env_info = env.reset(train_mode=mode)[env.brain_names[0]]\n",
    "    return env_info.vector_observations[0]\n",
    "\n",
    "def env_step(env, action):\n",
    "    env_info= env.step(action)[env.brain_names[0]]\n",
    "    return env_info.vector_observations[0], env_info.rewards[0], env_info.local_done[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "\n",
    "from ddpg import DDPG\n",
    "\n",
    "NUM_EPISODES = 300\n",
    "SOLVED_IN= 30\n",
    "\n",
    "start= datetime.now()\n",
    "\n",
    "state= env_reset(env, True)\n",
    "# Hyperparameters\n",
    "config= {\n",
    "    \"label\": \"Noise\",\n",
    "    \"state_size\": len(state),\n",
    "    \"action_size\": brain.vector_action_space_size,\n",
    "    \"seed\": seed,\n",
    "    \"actor_lr\": 0.001,\n",
    "    \"critic_lr\": 0.001,\n",
    "    \"actor_nodes\": [32, 32],\n",
    "    \"critic_nodes\": [128, 128],\n",
    "    \"batch_size\": 256,\n",
    "    \"memory_size\": 100000,\n",
    "    \"discount\": 0.9,\n",
    "    \"sigma\": 0.0, # OUNoise\n",
    "    \"tau\": 0.001,\n",
    "}\n",
    "\n",
    "\n",
    "agent = DDPG(config= config)\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    state= env_reset(env, True)\n",
    "    agent.reset()\n",
    "    ep_reward = 0\n",
    "    score= 0  \n",
    "    while True:\n",
    "        action = agent.act(state)    # Agent action. Include noise\n",
    "        next_state, reward, done= env_step(env, action)   # Environmet step\n",
    "        # Agent step. Includes learnig from memory\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        \n",
    "        score+= reward        # update the score\n",
    "        state= next_state     # roll over the state to next time step\n",
    "        if done:              # exit loop if episode finished\n",
    "            break\n",
    "    scores_window.append(score)       \n",
    "    agent.scores.append(score)              # save most recent score\n",
    "    mean_w_scores= np.mean(scores_window)\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}  '.format(episode+ 1, mean_w_scores), end=\"\")\n",
    "    if (episode+ 1) % 100 == 0:\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}  '.format(episode+ 1, mean_w_scores))\n",
    "    if mean_w_scores >= SOLVED_IN:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format((episode+ 1)-100, mean_w_scores))\n",
    "        break\n",
    "        \n",
    "agent.save()\n",
    "\n",
    "print('Elapsed time', datetime.now()- start)\n",
    "        \n",
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / N \n",
    "\n",
    "smoothed_scores= running_mean(agent.scores, 10)\n",
    "plt.plot(np.arange(len(smoothed_scores)), smoothed_scores)\n",
    "plt.plot(np.arange(len(agent.scores)), agent.scores, color='grey', alpha=0.5)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See how the agent behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ddpg import DDPG\n",
    "\n",
    "state= env_reset(env, False)\n",
    "config= {\n",
    "    \"label\": \"Noise\",\n",
    "    \"state_size\": len(state),\n",
    "    \"action_size\": brain.vector_action_space_size,\n",
    "    \"seed\": seed,\n",
    "    \"actor_lr\": 0.001,\n",
    "    \"critic_lr\": 0.001,\n",
    "    \"actor_nodes\": [32, 32],\n",
    "    \"critic_nodes\": [128, 128],\n",
    "    \"batch_size\": 256,\n",
    "    \"memory_size\": 100000,\n",
    "    \"discount\": 0.9,\n",
    "    \"sigma\": 0.0, # OUNoise\n",
    "    \"tau\": 0.001,\n",
    "}\n",
    "agent = DDPG(config= config)\n",
    "agent.actor.load_state_dict(torch.load(\"last_actor-aws.pth\", map_location=lambda storage, loc: storage))\n",
    "agent.actor.eval() \n",
    "\n",
    "score = 0                                          # initialize the score\n",
    "while True:\n",
    "    action = agent.act(state, False)\n",
    "    next_state, reward, done= env_step(env, action)\n",
    "        \n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    print('\\rScore: {:.1f} '.format(score), end=\"\")\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"\\rFinal score: {:.1f}\".format(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
